{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install opencv-python==4.6.0.66\n",
    "# # sudo apt install libgl1-mesa-glx -y\n",
    "# %pip install omegaconf==2.2.3\n",
    "# %pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "# %pip install accelerate==0.17.1\n",
    "# %pip install diffusers==0.13.1\n",
    "# %pip install huggingface-hub==0.14.1\n",
    "# %pip install transformers==4.27.4\n",
    "# %pip install einops==0.5.0\n",
    "# %pip install bitsandbytes==0.36.0.post2\n",
    "# %pip install triton==1.1.1\n",
    "# %pip install xformers==0.0.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/storygen/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: module 'triton.language' has no attribute 'constexpr'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from typing import Optional, Dict\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, DDIMScheduler\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, CLIPTextModel\n",
    "\n",
    "from utils.util import get_time_string, get_function_args\n",
    "from model.unet_2d_condition import UNet2DConditionModel\n",
    "from model.pipeline import StableDiffusionPipeline\n",
    "from dataset import StorySalonDataset\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "class SampleLogger:\n",
    "    def __init__(\n",
    "        self,\n",
    "        logdir: str,\n",
    "        subdir: str = \"sample\",\n",
    "        stage: str = 'no',\n",
    "        num_samples_per_prompt: int = 1,\n",
    "        num_inference_steps: int = 40,\n",
    "        guidance_scale: float = 7.0,\n",
    "        image_guidance_scale: float = 3.5,\n",
    "    ) -> None:\n",
    "        self.stage = stage\n",
    "        self.guidance_scale = guidance_scale\n",
    "        self.image_guidance_scale = image_guidance_scale\n",
    "        self.num_inference_steps = num_inference_steps\n",
    "        self.num_sample_per_prompt = num_samples_per_prompt\n",
    "        self.logdir = os.path.join(logdir, subdir)\n",
    "        os.makedirs(self.logdir)\n",
    "        \n",
    "    def log_sample_images(\n",
    "        self, batch, pipeline: StableDiffusionPipeline, device: torch.device, step: int\n",
    "    ):\n",
    "        sample_seeds = torch.randint(0, 100000, (self.num_sample_per_prompt,))\n",
    "        sample_seeds = sorted(sample_seeds.numpy().tolist())\n",
    "        self.sample_seeds = sample_seeds\n",
    "        self.prompts = batch[\"prompt\"]\n",
    "        self.prev_prompts = batch[\"ref_prompt\"]\n",
    "\n",
    "        for idx, prompt in enumerate(tqdm(self.prompts, desc=\"Generating sample images\")):\n",
    "            image = batch[\"image\"][idx, :, :, :].unsqueeze(0)\n",
    "            ref_images = batch[\"ref_image\"][idx, :, :, :, :].unsqueeze(0)\n",
    "            image = image.to(device=device)\n",
    "            ref_images = ref_images.to(device=device)\n",
    "            generator = []\n",
    "            for seed in self.sample_seeds:\n",
    "                generator_temp = torch.Generator(device=device)\n",
    "                generator_temp.manual_seed(seed)\n",
    "                generator.append(generator_temp) \n",
    "            sequence = pipeline(\n",
    "                stage = self.stage,\n",
    "                prompt = prompt,\n",
    "                image_prompt = ref_images,\n",
    "                prev_prompt = self.prev_prompts,\n",
    "                height=image.shape[2],\n",
    "                width=image.shape[3],\n",
    "                generator=generator,\n",
    "                num_inference_steps=self.num_inference_steps,\n",
    "                guidance_scale=self.guidance_scale,\n",
    "                image_guidance_scale = self.image_guidance_scale,\n",
    "                num_images_per_prompt=self.num_sample_per_prompt,\n",
    "            ).images\n",
    "\n",
    "            image = (image + 1.) / 2. # for visualization\n",
    "            image = image.squeeze().permute(1, 2, 0).detach().cpu().numpy()\n",
    "            cv2.imwrite(os.path.join(self.logdir, f\"{step}_{idx}_{seed}.png\"), image[:, :, ::-1] * 255)\n",
    "            v_refs = []\n",
    "            ref_images = ref_images.squeeze(0)\n",
    "            for ref_image in ref_images:\n",
    "                # v_ref = (ref_image + 1.) / 2. # for visualization\n",
    "                v_ref = ref_image.permute(1, 2, 0).detach().cpu().numpy()\n",
    "                v_refs.append(v_ref)\n",
    "            for i in range(len(v_refs)):\n",
    "                cv2.imwrite(os.path.join(self.logdir, f\"{step}_{idx}_{seed}_ref_{i}.png\"), v_refs[i][:, :, ::-1] * 255)\n",
    "                \n",
    "            with open(os.path.join(self.logdir, f\"{step}_{idx}_{seed}\" + '.txt'), 'a') as f:\n",
    "                f.write(batch['prompt'][idx])\n",
    "                f.write('\\n')\n",
    "                f.write('\\n')\n",
    "                for prev_prompt in self.prev_prompts:\n",
    "                    f.write(prev_prompt[0])\n",
    "                    f.write('\\n')\n",
    "            for i, img in enumerate(sequence):\n",
    "                img[0].save(os.path.join(self.logdir, f\"{step}_{idx}_{sample_seeds[i]}_output.png\"))\n",
    "            \n",
    "def train(\n",
    "    pretrained_model_path: str,\n",
    "    logdir: str,\n",
    "    train_steps: int = 300,\n",
    "    validation_steps: int = 1000,\n",
    "    validation_sample_logger: Optional[Dict] = None,\n",
    "    gradient_accumulation_steps: int = 30, # important hyper-parameter\n",
    "    seed: Optional[int] = None,\n",
    "    mixed_precision: Optional[str] = \"fp16\",\n",
    "    train_batch_size: int = 4,\n",
    "    val_batch_size: int = 1,\n",
    "    learning_rate: float = 3e-5,\n",
    "    scale_lr: bool = False,\n",
    "    lr_scheduler: str = \"constant\",  # [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"]\n",
    "    lr_warmup_steps: int = 0,\n",
    "    use_8bit_adam: bool = True,\n",
    "    adam_beta1: float = 0.9,\n",
    "    adam_beta2: float = 0.999,\n",
    "    adam_weight_decay: float = 1e-2,\n",
    "    adam_epsilon: float = 1e-08,\n",
    "    max_grad_norm: float = 1.0,\n",
    "    checkpointing_steps: int = 2000,\n",
    "):\n",
    "    \n",
    "    args = get_function_args()\n",
    "    time_string = get_time_string()\n",
    "    logdir += f\"_{time_string}\"\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        mixed_precision=mixed_precision,\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        os.makedirs(logdir, exist_ok=True)\n",
    "        OmegaConf.save(args, os.path.join(logdir, \"config.yml\"))\n",
    "\n",
    "    if seed is not None:\n",
    "        set_seed(seed)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_path, subfolder=\"tokenizer\", use_fast=False)\n",
    "    text_encoder = CLIPTextModel.from_pretrained(pretrained_model_path, subfolder=\"text_encoder\")\n",
    "    vae = AutoencoderKL.from_pretrained(pretrained_model_path, subfolder=\"vae\")\n",
    "    unet = UNet2DConditionModel.from_config(pretrained_model_path, subfolder=\"unet\")\n",
    "    pretrained_sdm = torch.load('/workspace/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin', map_location='cpu')\n",
    "    unet.load_SDM_state_dict(pretrained_sdm)\n",
    "    # unet = UNet2DConditionModel.from_pretrained(pretrained_model_path, subfolder=\"unet\")\n",
    "    scheduler = DDIMScheduler.from_pretrained(pretrained_model_path, subfolder=\"scheduler\")\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_path, subfolder=\"scheduler\")\n",
    "    \n",
    "    pipeline = StableDiffusionPipeline(\n",
    "        vae=vae,\n",
    "        text_encoder=text_encoder,\n",
    "        tokenizer=tokenizer,\n",
    "        unet=unet,\n",
    "        scheduler=scheduler,\n",
    "    )\n",
    "    pipeline.set_progress_bar_config(disable=True)\n",
    "\n",
    "    if is_xformers_available():\n",
    "        try:\n",
    "            pipeline.enable_xformers_memory_efficient_attention()\n",
    "        except Exception as e:\n",
    "            logger.warning(\n",
    "                \"Could not enable memory efficient attention. Make sure xformers is installed\"\n",
    "                f\" correctly and a GPU is available: {e}\"\n",
    "            )\n",
    "    \n",
    "    vae.requires_grad_(False)\n",
    "    text_encoder.requires_grad_(False)\n",
    "    unet.requires_grad_(False)\n",
    "\n",
    "    trainable_modules = (\"attn1\")\n",
    "    for name, module in unet.named_modules():\n",
    "        if name.endswith(trainable_modules):\n",
    "            for params in module.parameters():\n",
    "                params.requires_grad = True\n",
    "        \n",
    "    if scale_lr:\n",
    "        learning_rate = (\n",
    "            learning_rate * gradient_accumulation_steps * train_batch_size * accelerator.num_processes\n",
    "        )\n",
    "\n",
    "    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\n",
    "    if use_8bit_adam:\n",
    "        try:\n",
    "            import bitsandbytes as bnb\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\"\n",
    "            )\n",
    "        optimizer_class = bnb.optim.AdamW8bit\n",
    "    else:\n",
    "        optimizer_class = torch.optim.AdamW\n",
    "\n",
    "    params_to_optimize = unet.parameters()\n",
    "    optimizer = optimizer_class(\n",
    "        params_to_optimize,\n",
    "        lr=learning_rate,\n",
    "        betas=(adam_beta1, adam_beta2),\n",
    "        weight_decay=adam_weight_decay,\n",
    "        eps=adam_epsilon,\n",
    "    )\n",
    "\n",
    "    train_dataset = StorySalonDataset(root=\"./StorySalon/\", dataset_name='train')\n",
    "    val_dataset = StorySalonDataset(root=\"./StorySalon/\", dataset_name='test')\n",
    "    \n",
    "    print(train_dataset.__len__())\n",
    "    print(val_dataset.__len__())\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=8)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        lr_scheduler,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=lr_warmup_steps * gradient_accumulation_steps,\n",
    "        num_training_steps=train_steps * gradient_accumulation_steps,\n",
    "    )\n",
    "\n",
    "    unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        unet, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    weight_dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        weight_dtype = torch.bfloat16\n",
    "\n",
    "    # For mixed precision training we cast the text_encoder and vae weights to half-precision\n",
    "    # as these models are only used for inference, keeping weights in full precision is not required.\n",
    "    vae.to(accelerator.device, dtype=weight_dtype)\n",
    "    text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "    \n",
    "    # We need to initialize the trackers we use, and also store our configuration.\n",
    "    # The trackers initializes automatically on the main process.\n",
    "    if accelerator.is_main_process:\n",
    "        accelerator.init_trackers(\"StoryGen-Single\")\n",
    "    step = 0\n",
    "\n",
    "    if validation_sample_logger is not None and accelerator.is_main_process:\n",
    "        validation_sample_logger = SampleLogger(**validation_sample_logger, logdir=logdir)\n",
    "\n",
    "    progress_bar = tqdm(range(step, train_steps), disable=not accelerator.is_local_main_process)\n",
    "    progress_bar.set_description(\"Steps\")\n",
    "\n",
    "    def make_data_yielder(dataloader):\n",
    "        while True:\n",
    "            for batch in dataloader:\n",
    "                yield batch\n",
    "            accelerator.wait_for_everyone()\n",
    "\n",
    "    train_data_yielder = make_data_yielder(train_dataloader)\n",
    "    val_data_yielder = make_data_yielder(val_dataloader)\n",
    "\n",
    "    while step < train_steps:\n",
    "        batch = next(train_data_yielder)\n",
    "        \n",
    "        vae.eval()\n",
    "        text_encoder.eval()\n",
    "        unet.train()\n",
    "        \n",
    "        image = batch[\"image\"].to(dtype=weight_dtype)\n",
    "        prompt = batch[\"prompt\"]\n",
    "        prompt_ids = tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=tokenizer.model_max_length, return_tensors=\"pt\").input_ids\n",
    "        mask = batch[\"mask\"].to(dtype=weight_dtype)        \n",
    "        mask = mask[:, [0], :, :].repeat(1, 4, 1, 1) # 3 channels to 4 channels\n",
    "        mask = F.interpolate(mask, scale_factor = 1 / 8., mode=\"bilinear\", align_corners=False)\n",
    "        b, c, h, w = image.shape\n",
    "        \n",
    "        latents = vae.encode(image).latent_dist.sample()\n",
    "        latents = latents * 0.18215\n",
    "        \n",
    "        # Sample noise that we'll add\n",
    "        noise = torch.randn_like(latents) # [-1, 1]\n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (b,), device=latents.device)\n",
    "        timesteps = timesteps.long()\n",
    "        \n",
    "        # Add noise according to the noise magnitude at each timestep (this is the forward diffusion process)\n",
    "        noisy_latent = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "        # Get the text embedding for conditioning\n",
    "        encoder_hidden_states = text_encoder(prompt_ids.to(accelerator.device))[0] # B * 77 * 768\n",
    "        \n",
    "        # Predict the noise residual\n",
    "        model_pred = unet(noisy_latent, timesteps, encoder_hidden_states=encoder_hidden_states, image_hidden_states=None, return_dict=False)[0]\n",
    "        \n",
    "        # loss = F.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
    "        loss = F.mse_loss(model_pred.float() * (1. - mask), noise.float() * (1 - mask), reduction=\"mean\")\n",
    "\n",
    "        accelerator.backward(loss)\n",
    "        if accelerator.sync_gradients:\n",
    "            accelerator.clip_grad_norm_(unet.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if accelerator.sync_gradients:\n",
    "            progress_bar.update(1)\n",
    "            step += 1\n",
    "            if accelerator.is_main_process:\n",
    "                if validation_sample_logger is not None and step % validation_steps == 0:\n",
    "                    unet.eval()\n",
    "                    val_batch = next(val_data_yielder)\n",
    "                    with autocast():\n",
    "                        validation_sample_logger.log_sample_images(\n",
    "                            batch = val_batch,\n",
    "                            pipeline=pipeline,\n",
    "                            device=accelerator.device,\n",
    "                            step=step,\n",
    "                        )\n",
    "                if step % checkpointing_steps == 0:\n",
    "                    pipeline_save = StableDiffusionPipeline(\n",
    "                        vae=vae,\n",
    "                        text_encoder=text_encoder,\n",
    "                        tokenizer=tokenizer,\n",
    "                        unet=accelerator.unwrap_model(unet),\n",
    "                        scheduler=scheduler,\n",
    "                    )\n",
    "                    checkpoint_save_path = os.path.join(logdir, f\"checkpoint_{step}\")\n",
    "                    pipeline_save.save_pretrained(checkpoint_save_path)\n",
    "\n",
    "        logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        accelerator.log(logs, step=step)\n",
    "    accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/storygen/lib/python3.8/site-packages/diffusers/configuration_utils.py:195: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'model.unet_2d_condition.UNet2DConditionModel'>.load_config(...) followed by <class 'model.unet_2d_condition.UNet2DConditionModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
      "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "The config attributes {'center_input_sample': False} were passed to UNet2DConditionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_dict key down_blocks.0.attentions.0.transformer_blocks.0.attn3.to_q.weight is initialized with self attention.\n",
      "state_dict key down_blocks.0.attentions.0.transformer_blocks.0.attn3.to_k.weight is initialized with self attention.\n",
      "state_dict key down_blocks.0.attentions.0.transformer_blocks.0.attn3.to_v.weight is initialized with self attention.\n",
      "state_dict key down_blocks.0.attentions.0.transformer_blocks.0.attn3.to_out.0.weight is initialized with self attention.\n",
      "state_dict key down_blocks.0.attentions.0.transformer_blocks.0.attn3.to_out.0.bias is initialized with self attention.\n",
      "state_dict key down_blocks.0.attentions.0.transformer_blocks.0.norm4.weight is initialized with self attention.\n",
      "state_dict key down_blocks.0.attentions.0.transformer_blocks.0.norm4.bias is initialized with self attention.\n",
      "state_dict key down_blocks.0.attentions.1.transformer_blocks.0.attn3.to_q.weight is initialized with self attention.\n",
      "state_dict key down_blocks.0.attentions.1.transformer_blocks.0.attn3.to_k.weight is initialized with self attention.\n",
      "state_dict key down_blocks.0.attentions.1.transformer_blocks.0.attn3.to_v.weight is initialized with self attention.\n",
      "state_dict key down_blocks.0.attentions.1.transformer_blocks.0.attn3.to_out.0.weight is initialized with self attention.\n",
      "state_dict key down_blocks.0.attentions.1.transformer_blocks.0.attn3.to_out.0.bias is initialized with self attention.\n",
      "state_dict key down_blocks.0.attentions.1.transformer_blocks.0.norm4.weight is initialized with self attention.\n",
      "state_dict key down_blocks.0.attentions.1.transformer_blocks.0.norm4.bias is initialized with self attention.\n",
      "state_dict key down_blocks.1.attentions.0.transformer_blocks.0.attn3.to_q.weight is initialized with self attention.\n",
      "state_dict key down_blocks.1.attentions.0.transformer_blocks.0.attn3.to_k.weight is initialized with self attention.\n",
      "state_dict key down_blocks.1.attentions.0.transformer_blocks.0.attn3.to_v.weight is initialized with self attention.\n",
      "state_dict key down_blocks.1.attentions.0.transformer_blocks.0.attn3.to_out.0.weight is initialized with self attention.\n",
      "state_dict key down_blocks.1.attentions.0.transformer_blocks.0.attn3.to_out.0.bias is initialized with self attention.\n",
      "state_dict key down_blocks.1.attentions.0.transformer_blocks.0.norm4.weight is initialized with self attention.\n",
      "state_dict key down_blocks.1.attentions.0.transformer_blocks.0.norm4.bias is initialized with self attention.\n",
      "state_dict key down_blocks.1.attentions.1.transformer_blocks.0.attn3.to_q.weight is initialized with self attention.\n",
      "state_dict key down_blocks.1.attentions.1.transformer_blocks.0.attn3.to_k.weight is initialized with self attention.\n",
      "state_dict key down_blocks.1.attentions.1.transformer_blocks.0.attn3.to_v.weight is initialized with self attention.\n",
      "state_dict key down_blocks.1.attentions.1.transformer_blocks.0.attn3.to_out.0.weight is initialized with self attention.\n",
      "state_dict key down_blocks.1.attentions.1.transformer_blocks.0.attn3.to_out.0.bias is initialized with self attention.\n",
      "state_dict key down_blocks.1.attentions.1.transformer_blocks.0.norm4.weight is initialized with self attention.\n",
      "state_dict key down_blocks.1.attentions.1.transformer_blocks.0.norm4.bias is initialized with self attention.\n",
      "state_dict key down_blocks.2.attentions.0.transformer_blocks.0.attn3.to_q.weight is initialized with self attention.\n",
      "state_dict key down_blocks.2.attentions.0.transformer_blocks.0.attn3.to_k.weight is initialized with self attention.\n",
      "state_dict key down_blocks.2.attentions.0.transformer_blocks.0.attn3.to_v.weight is initialized with self attention.\n",
      "state_dict key down_blocks.2.attentions.0.transformer_blocks.0.attn3.to_out.0.weight is initialized with self attention.\n",
      "state_dict key down_blocks.2.attentions.0.transformer_blocks.0.attn3.to_out.0.bias is initialized with self attention.\n",
      "state_dict key down_blocks.2.attentions.0.transformer_blocks.0.norm4.weight is initialized with self attention.\n",
      "state_dict key down_blocks.2.attentions.0.transformer_blocks.0.norm4.bias is initialized with self attention.\n",
      "state_dict key down_blocks.2.attentions.1.transformer_blocks.0.attn3.to_q.weight is initialized with self attention.\n",
      "state_dict key down_blocks.2.attentions.1.transformer_blocks.0.attn3.to_k.weight is initialized with self attention.\n",
      "state_dict key down_blocks.2.attentions.1.transformer_blocks.0.attn3.to_v.weight is initialized with self attention.\n",
      "state_dict key down_blocks.2.attentions.1.transformer_blocks.0.attn3.to_out.0.weight is initialized with self attention.\n",
      "state_dict key down_blocks.2.attentions.1.transformer_blocks.0.attn3.to_out.0.bias is initialized with self attention.\n",
      "state_dict key down_blocks.2.attentions.1.transformer_blocks.0.norm4.weight is initialized with self attention.\n",
      "state_dict key down_blocks.2.attentions.1.transformer_blocks.0.norm4.bias is initialized with self attention.\n",
      "state_dict key up_blocks.1.attentions.0.transformer_blocks.0.attn3.to_q.weight is initialized with self attention.\n",
      "state_dict key up_blocks.1.attentions.0.transformer_blocks.0.attn3.to_k.weight is initialized with self attention.\n",
      "state_dict key up_blocks.1.attentions.0.transformer_blocks.0.attn3.to_v.weight is initialized with self attention.\n",
      "state_dict key up_blocks.1.attentions.0.transformer_blocks.0.attn3.to_out.0.weight is initialized with self attention.\n",
      "state_dict key up_blocks.1.attentions.0.transformer_blocks.0.attn3.to_out.0.bias is initialized with self attention.\n",
      "state_dict key up_blocks.1.attentions.0.transformer_blocks.0.norm4.weight is initialized with self attention.\n",
      "state_dict key up_blocks.1.attentions.0.transformer_blocks.0.norm4.bias is initialized with self attention.\n",
      "state_dict key up_blocks.1.attentions.1.transformer_blocks.0.attn3.to_q.weight is initialized with self attention.\n",
      "state_dict key up_blocks.1.attentions.1.transformer_blocks.0.attn3.to_k.weight is initialized with self attention.\n",
      "state_dict key up_blocks.1.attentions.1.transformer_blocks.0.attn3.to_v.weight is initialized with self attention.\n",
      "state_dict key up_blocks.1.attentions.1.transformer_blocks.0.attn3.to_out.0.weight is initialized with self attention.\n",
      "state_dict key up_blocks.1.attentions.1.transformer_blocks.0.attn3.to_out.0.bias is initialized with self attention.\n",
      "state_dict key up_blocks.1.attentions.1.transformer_blocks.0.norm4.weight is initialized with self attention.\n",
      "state_dict key up_blocks.1.attentions.1.transformer_blocks.0.norm4.bias is initialized with self attention.\n",
      "state_dict key up_blocks.1.attentions.2.transformer_blocks.0.attn3.to_q.weight is initialized with self attention.\n",
      "state_dict key up_blocks.1.attentions.2.transformer_blocks.0.attn3.to_k.weight is initialized with self attention.\n",
      "state_dict key up_blocks.1.attentions.2.transformer_blocks.0.attn3.to_v.weight is initialized with self attention.\n",
      "state_dict key up_blocks.1.attentions.2.transformer_blocks.0.attn3.to_out.0.weight is initialized with self attention.\n",
      "state_dict key up_blocks.1.attentions.2.transformer_blocks.0.attn3.to_out.0.bias is initialized with self attention.\n",
      "state_dict key up_blocks.1.attentions.2.transformer_blocks.0.norm4.weight is initialized with self attention.\n",
      "state_dict key up_blocks.1.attentions.2.transformer_blocks.0.norm4.bias is initialized with self attention.\n",
      "state_dict key up_blocks.2.attentions.0.transformer_blocks.0.attn3.to_q.weight is initialized with self attention.\n",
      "state_dict key up_blocks.2.attentions.0.transformer_blocks.0.attn3.to_k.weight is initialized with self attention.\n",
      "state_dict key up_blocks.2.attentions.0.transformer_blocks.0.attn3.to_v.weight is initialized with self attention.\n",
      "state_dict key up_blocks.2.attentions.0.transformer_blocks.0.attn3.to_out.0.weight is initialized with self attention.\n",
      "state_dict key up_blocks.2.attentions.0.transformer_blocks.0.attn3.to_out.0.bias is initialized with self attention.\n",
      "state_dict key up_blocks.2.attentions.0.transformer_blocks.0.norm4.weight is initialized with self attention.\n",
      "state_dict key up_blocks.2.attentions.0.transformer_blocks.0.norm4.bias is initialized with self attention.\n",
      "state_dict key up_blocks.2.attentions.1.transformer_blocks.0.attn3.to_q.weight is initialized with self attention.\n",
      "state_dict key up_blocks.2.attentions.1.transformer_blocks.0.attn3.to_k.weight is initialized with self attention.\n",
      "state_dict key up_blocks.2.attentions.1.transformer_blocks.0.attn3.to_v.weight is initialized with self attention.\n",
      "state_dict key up_blocks.2.attentions.1.transformer_blocks.0.attn3.to_out.0.weight is initialized with self attention.\n",
      "state_dict key up_blocks.2.attentions.1.transformer_blocks.0.attn3.to_out.0.bias is initialized with self attention.\n",
      "state_dict key up_blocks.2.attentions.1.transformer_blocks.0.norm4.weight is initialized with self attention.\n",
      "state_dict key up_blocks.2.attentions.1.transformer_blocks.0.norm4.bias is initialized with self attention.\n",
      "state_dict key up_blocks.2.attentions.2.transformer_blocks.0.attn3.to_q.weight is initialized with self attention.\n",
      "state_dict key up_blocks.2.attentions.2.transformer_blocks.0.attn3.to_k.weight is initialized with self attention.\n",
      "state_dict key up_blocks.2.attentions.2.transformer_blocks.0.attn3.to_v.weight is initialized with self attention.\n",
      "state_dict key up_blocks.2.attentions.2.transformer_blocks.0.attn3.to_out.0.weight is initialized with self attention.\n",
      "state_dict key up_blocks.2.attentions.2.transformer_blocks.0.attn3.to_out.0.bias is initialized with self attention.\n",
      "state_dict key up_blocks.2.attentions.2.transformer_blocks.0.norm4.weight is initialized with self attention.\n",
      "state_dict key up_blocks.2.attentions.2.transformer_blocks.0.norm4.bias is initialized with self attention.\n",
      "state_dict key up_blocks.3.attentions.0.transformer_blocks.0.attn3.to_q.weight is initialized with self attention.\n",
      "state_dict key up_blocks.3.attentions.0.transformer_blocks.0.attn3.to_k.weight is initialized with self attention.\n",
      "state_dict key up_blocks.3.attentions.0.transformer_blocks.0.attn3.to_v.weight is initialized with self attention.\n",
      "state_dict key up_blocks.3.attentions.0.transformer_blocks.0.attn3.to_out.0.weight is initialized with self attention.\n",
      "state_dict key up_blocks.3.attentions.0.transformer_blocks.0.attn3.to_out.0.bias is initialized with self attention.\n",
      "state_dict key up_blocks.3.attentions.0.transformer_blocks.0.norm4.weight is initialized with self attention.\n",
      "state_dict key up_blocks.3.attentions.0.transformer_blocks.0.norm4.bias is initialized with self attention.\n",
      "state_dict key up_blocks.3.attentions.1.transformer_blocks.0.attn3.to_q.weight is initialized with self attention.\n",
      "state_dict key up_blocks.3.attentions.1.transformer_blocks.0.attn3.to_k.weight is initialized with self attention.\n",
      "state_dict key up_blocks.3.attentions.1.transformer_blocks.0.attn3.to_v.weight is initialized with self attention.\n",
      "state_dict key up_blocks.3.attentions.1.transformer_blocks.0.attn3.to_out.0.weight is initialized with self attention.\n",
      "state_dict key up_blocks.3.attentions.1.transformer_blocks.0.attn3.to_out.0.bias is initialized with self attention.\n",
      "state_dict key up_blocks.3.attentions.1.transformer_blocks.0.norm4.weight is initialized with self attention.\n",
      "state_dict key up_blocks.3.attentions.1.transformer_blocks.0.norm4.bias is initialized with self attention.\n",
      "state_dict key up_blocks.3.attentions.2.transformer_blocks.0.attn3.to_q.weight is initialized with self attention.\n",
      "state_dict key up_blocks.3.attentions.2.transformer_blocks.0.attn3.to_k.weight is initialized with self attention.\n",
      "state_dict key up_blocks.3.attentions.2.transformer_blocks.0.attn3.to_v.weight is initialized with self attention.\n",
      "state_dict key up_blocks.3.attentions.2.transformer_blocks.0.attn3.to_out.0.weight is initialized with self attention.\n",
      "state_dict key up_blocks.3.attentions.2.transformer_blocks.0.attn3.to_out.0.bias is initialized with self attention.\n",
      "state_dict key up_blocks.3.attentions.2.transformer_blocks.0.norm4.weight is initialized with self attention.\n",
      "state_dict key up_blocks.3.attentions.2.transformer_blocks.0.norm4.bias is initialized with self attention.\n",
      "state_dict key mid_block.attentions.0.transformer_blocks.0.attn3.to_q.weight is initialized with self attention.\n",
      "state_dict key mid_block.attentions.0.transformer_blocks.0.attn3.to_k.weight is initialized with self attention.\n",
      "state_dict key mid_block.attentions.0.transformer_blocks.0.attn3.to_v.weight is initialized with self attention.\n",
      "state_dict key mid_block.attentions.0.transformer_blocks.0.attn3.to_out.0.weight is initialized with self attention.\n",
      "state_dict key mid_block.attentions.0.transformer_blocks.0.attn3.to_out.0.bias is initialized with self attention.\n",
      "state_dict key mid_block.attentions.0.transformer_blocks.0.norm4.weight is initialized with self attention.\n",
      "state_dict key mid_block.attentions.0.transformer_blocks.0.norm4.bias is initialized with self attention.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:bitsandbytes.cextension:Could not find the bitsandbytes CUDA binary at PosixPath('/root/miniconda3/envs/storygen/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda116.so')\n",
      "WARNING:bitsandbytes.cextension:The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'triton.language' has no attribute 'constexpr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/workspace/StoryGen/config/stage1_config_server.yml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mOmegaConf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# CUDA_VISIBLE_DEVICES=0,1 accelerate launch train_StorySalon.py\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 189\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(pretrained_model_path, logdir, train_steps, validation_steps, validation_sample_logger, gradient_accumulation_steps, seed, mixed_precision, train_batch_size, val_batch_size, learning_rate, scale_lr, lr_scheduler, lr_warmup_steps, use_8bit_adam, adam_beta1, adam_beta2, adam_weight_decay, adam_epsilon, max_grad_norm, checkpointing_steps)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_8bit_adam:\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 189\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mbnb\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    192\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/storygen/lib/python3.8/site-packages/bitsandbytes/__init__.py:15\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m research, utils\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     MatmulLtState,\n\u001b[1;32m      9\u001b[0m     bmm_cublas,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     mm_cublas,\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m modules\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adam\n\u001b[1;32m     18\u001b[0m __pdoc__ \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptim.optimizer.Optimizer8bit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptim.optimizer.MockArgs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     22\u001b[0m }\n",
      "File \u001b[0;32m~/miniconda3/envs/storygen/lib/python3.8/site-packages/bitsandbytes/nn/__init__.py:21\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     Embedding,\n\u001b[1;32m      7\u001b[0m     Embedding4bit,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     SwitchBackLinearBnb,\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtriton_based_modules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     StandardLinear,\n\u001b[1;32m     23\u001b[0m     SwitchBackLinear,\n\u001b[1;32m     24\u001b[0m     SwitchBackLinearGlobal,\n\u001b[1;32m     25\u001b[0m     SwitchBackLinearVectorwise,\n\u001b[1;32m     26\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/storygen/lib/python3.8/site-packages/bitsandbytes/nn/triton_based_modules.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtriton\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdequantize_rowwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dequantize_rowwise\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtriton\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mint8_matmul_mixed_dequantize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     int8_matmul_mixed_dequantize,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtriton\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mint8_matmul_rowwise_dequantize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     int8_matmul_rowwise_dequantize,\n\u001b[1;32m     12\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/storygen/lib/python3.8/site-packages/bitsandbytes/triton/dequantize_rowwise.py:42\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtriton\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtl\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# rowwise quantize\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# TODO: autotune this better.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;129m@triton\u001b[39m\u001b[38;5;241m.\u001b[39mautotune(\n\u001b[1;32m     19\u001b[0m     configs\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     20\u001b[0m         triton\u001b[38;5;241m.\u001b[39mConfig({}, num_stages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, num_warps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m),\n\u001b[1;32m     21\u001b[0m         triton\u001b[38;5;241m.\u001b[39mConfig({}, num_stages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, num_warps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m),\n\u001b[1;32m     22\u001b[0m         triton\u001b[38;5;241m.\u001b[39mConfig({}, num_stages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_warps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m),\n\u001b[1;32m     23\u001b[0m         triton\u001b[38;5;241m.\u001b[39mConfig({}, num_stages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, num_warps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m),\n\u001b[1;32m     24\u001b[0m         triton\u001b[38;5;241m.\u001b[39mConfig({}, num_stages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     25\u001b[0m         triton\u001b[38;5;241m.\u001b[39mConfig({}, num_stages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     26\u001b[0m         triton\u001b[38;5;241m.\u001b[39mConfig({}, num_stages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m     27\u001b[0m         triton\u001b[38;5;241m.\u001b[39mConfig({}, num_stages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m),\n\u001b[1;32m     28\u001b[0m         triton\u001b[38;5;241m.\u001b[39mConfig({}, num_warps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     29\u001b[0m         triton\u001b[38;5;241m.\u001b[39mConfig({}, num_warps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     30\u001b[0m         triton\u001b[38;5;241m.\u001b[39mConfig({}, num_warps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m     31\u001b[0m         triton\u001b[38;5;241m.\u001b[39mConfig({}, num_warps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m),\n\u001b[1;32m     32\u001b[0m     ],\n\u001b[1;32m     33\u001b[0m     key\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_elements\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m \u001b[38;5;129m@triton\u001b[39m\u001b[38;5;241m.\u001b[39mjit\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_dequantize_rowwise\u001b[39m(\n\u001b[1;32m     37\u001b[0m     x_ptr,\n\u001b[1;32m     38\u001b[0m     state_x,\n\u001b[1;32m     39\u001b[0m     output_ptr,\n\u001b[1;32m     40\u001b[0m     inv_127,\n\u001b[1;32m     41\u001b[0m     n_elements,\n\u001b[0;32m---> 42\u001b[0m     BLOCK_SIZE: \u001b[43mtl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstexpr\u001b[49m,\n\u001b[1;32m     43\u001b[0m     P2: tl\u001b[38;5;241m.\u001b[39mconstexpr,\n\u001b[1;32m     44\u001b[0m ):\n\u001b[1;32m     45\u001b[0m     pid \u001b[38;5;241m=\u001b[39m tl\u001b[38;5;241m.\u001b[39mprogram_id(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     46\u001b[0m     block_start \u001b[38;5;241m=\u001b[39m pid \u001b[38;5;241m*\u001b[39m BLOCK_SIZE\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'triton.language' has no attribute 'constexpr'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = \"/workspace/StoryGen/config/stage1_config_server.yml\"\n",
    "    train(**OmegaConf.load(config))\n",
    "\n",
    "# CUDA_VISIBLE_DEVICES=0,1 accelerate launch train_StorySalon.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "storygen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
