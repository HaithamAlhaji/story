{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install opencv-python==4.6.0.66\n",
    "# # sudo apt install libgl1-mesa-glx -y\n",
    "# %pip install omegaconf==2.2.3\n",
    "# %pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "# %pip install accelerate==0.17.1\n",
    "# %pip install diffusers==0.13.1\n",
    "# %pip install huggingface-hub==0.14.1\n",
    "# %pip install transformers==4.27.4\n",
    "# %pip install einops==0.5.0\n",
    "# %pip install bitsandbytes==0.36.0.post2\n",
    "# %pip install triton==1.1.1\n",
    "# %pip install xformers==0.0.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from typing import Optional, Dict\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, DDIMScheduler\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, CLIPTextModel\n",
    "\n",
    "from utils.util import get_time_string, get_function_args\n",
    "from model.unet_2d_condition import UNet2DConditionModel\n",
    "from model.pipeline import StableDiffusionPipeline\n",
    "from dataset import StorySalonDataset\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "class SampleLogger:\n",
    "    def __init__(\n",
    "        self,\n",
    "        logdir: str,\n",
    "        subdir: str = \"sample\",\n",
    "        stage: str = 'no',\n",
    "        num_samples_per_prompt: int = 1,\n",
    "        num_inference_steps: int = 40,\n",
    "        guidance_scale: float = 7.0,\n",
    "        image_guidance_scale: float = 3.5,\n",
    "    ) -> None:\n",
    "        self.stage = stage\n",
    "        self.guidance_scale = guidance_scale\n",
    "        self.image_guidance_scale = image_guidance_scale\n",
    "        self.num_inference_steps = num_inference_steps\n",
    "        self.num_sample_per_prompt = num_samples_per_prompt\n",
    "        self.logdir = os.path.join(logdir, subdir)\n",
    "        os.makedirs(self.logdir)\n",
    "        \n",
    "    def log_sample_images(\n",
    "        self, batch, pipeline: StableDiffusionPipeline, device: torch.device, step: int\n",
    "    ):\n",
    "        sample_seeds = torch.randint(0, 100000, (self.num_sample_per_prompt,))\n",
    "        sample_seeds = sorted(sample_seeds.numpy().tolist())\n",
    "        self.sample_seeds = sample_seeds\n",
    "        self.prompts = batch[\"prompt\"]\n",
    "        self.prev_prompts = batch[\"ref_prompt\"]\n",
    "\n",
    "        for idx, prompt in enumerate(tqdm(self.prompts, desc=\"Generating sample images\")):\n",
    "            image = batch[\"image\"][idx, :, :, :].unsqueeze(0)\n",
    "            ref_images = batch[\"ref_image\"][idx, :, :, :, :].unsqueeze(0)\n",
    "            image = image.to(device=device)\n",
    "            ref_images = ref_images.to(device=device)\n",
    "            generator = []\n",
    "            for seed in self.sample_seeds:\n",
    "                generator_temp = torch.Generator(device=device)\n",
    "                generator_temp.manual_seed(seed)\n",
    "                generator.append(generator_temp) \n",
    "            sequence = pipeline(\n",
    "                stage = self.stage,\n",
    "                prompt = prompt,\n",
    "                image_prompt = ref_images,\n",
    "                prev_prompt = self.prev_prompts,\n",
    "                height=image.shape[2],\n",
    "                width=image.shape[3],\n",
    "                generator=generator,\n",
    "                num_inference_steps=self.num_inference_steps,\n",
    "                guidance_scale=self.guidance_scale,\n",
    "                image_guidance_scale = self.image_guidance_scale,\n",
    "                num_images_per_prompt=self.num_sample_per_prompt,\n",
    "            ).images\n",
    "\n",
    "            image = (image + 1.) / 2. # for visualization\n",
    "            image = image.squeeze().permute(1, 2, 0).detach().cpu().numpy()\n",
    "            cv2.imwrite(os.path.join(self.logdir, f\"{step}_{idx}_{seed}.png\"), image[:, :, ::-1] * 255)\n",
    "            v_refs = []\n",
    "            ref_images = ref_images.squeeze(0)\n",
    "            for ref_image in ref_images:\n",
    "                # v_ref = (ref_image + 1.) / 2. # for visualization\n",
    "                v_ref = ref_image.permute(1, 2, 0).detach().cpu().numpy()\n",
    "                v_refs.append(v_ref)\n",
    "            for i in range(len(v_refs)):\n",
    "                cv2.imwrite(os.path.join(self.logdir, f\"{step}_{idx}_{seed}_ref_{i}.png\"), v_refs[i][:, :, ::-1] * 255)\n",
    "                \n",
    "            with open(os.path.join(self.logdir, f\"{step}_{idx}_{seed}\" + '.txt'), 'a') as f:\n",
    "                f.write(batch['prompt'][idx])\n",
    "                f.write('\\n')\n",
    "                f.write('\\n')\n",
    "                for prev_prompt in self.prev_prompts:\n",
    "                    f.write(prev_prompt[0])\n",
    "                    f.write('\\n')\n",
    "            for i, img in enumerate(sequence):\n",
    "                img[0].save(os.path.join(self.logdir, f\"{step}_{idx}_{sample_seeds[i]}_output.png\"))\n",
    "            \n",
    "def train(\n",
    "    pretrained_model_path: str,\n",
    "    logdir: str,\n",
    "    train_steps: int = 300,\n",
    "    validation_steps: int = 1000,\n",
    "    validation_sample_logger: Optional[Dict] = None,\n",
    "    gradient_accumulation_steps: int = 30, # important hyper-parameter\n",
    "    seed: Optional[int] = None,\n",
    "    mixed_precision: Optional[str] = \"fp16\",\n",
    "    train_batch_size: int = 4,\n",
    "    val_batch_size: int = 1,\n",
    "    learning_rate: float = 3e-5,\n",
    "    scale_lr: bool = False,\n",
    "    lr_scheduler: str = \"constant\",  # [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"]\n",
    "    lr_warmup_steps: int = 0,\n",
    "    use_8bit_adam: bool = True,\n",
    "    adam_beta1: float = 0.9,\n",
    "    adam_beta2: float = 0.999,\n",
    "    adam_weight_decay: float = 1e-2,\n",
    "    adam_epsilon: float = 1e-08,\n",
    "    max_grad_norm: float = 1.0,\n",
    "    checkpointing_steps: int = 2000,\n",
    "):\n",
    "    \n",
    "    args = get_function_args()\n",
    "    time_string = get_time_string()\n",
    "    logdir += f\"_{time_string}\"\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        mixed_precision=mixed_precision,\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        os.makedirs(logdir, exist_ok=True)\n",
    "        OmegaConf.save(args, os.path.join(logdir, \"config.yml\"))\n",
    "\n",
    "    if seed is not None:\n",
    "        set_seed(seed)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_path, subfolder=\"tokenizer\", use_fast=False)\n",
    "    text_encoder = CLIPTextModel.from_pretrained(pretrained_model_path, subfolder=\"text_encoder\")\n",
    "    vae = AutoencoderKL.from_pretrained(pretrained_model_path, subfolder=\"vae\")\n",
    "    unet = UNet2DConditionModel.from_config(pretrained_model_path, subfolder=\"unet\")\n",
    "    pretrained_sdm = torch.load('/workspace/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin', map_location='cpu')\n",
    "    unet.load_SDM_state_dict(pretrained_sdm)\n",
    "    # unet = UNet2DConditionModel.from_pretrained(pretrained_model_path, subfolder=\"unet\")\n",
    "    scheduler = DDIMScheduler.from_pretrained(pretrained_model_path, subfolder=\"scheduler\")\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_path, subfolder=\"scheduler\")\n",
    "    \n",
    "    pipeline = StableDiffusionPipeline(\n",
    "        vae=vae,\n",
    "        text_encoder=text_encoder,\n",
    "        tokenizer=tokenizer,\n",
    "        unet=unet,\n",
    "        scheduler=scheduler,\n",
    "    )\n",
    "    pipeline.set_progress_bar_config(disable=True)\n",
    "\n",
    "    if is_xformers_available():\n",
    "        try:\n",
    "            pipeline.enable_xformers_memory_efficient_attention()\n",
    "        except Exception as e:\n",
    "            logger.warning(\n",
    "                \"Could not enable memory efficient attention. Make sure xformers is installed\"\n",
    "                f\" correctly and a GPU is available: {e}\"\n",
    "            )\n",
    "    \n",
    "    vae.requires_grad_(False)\n",
    "    text_encoder.requires_grad_(False)\n",
    "    unet.requires_grad_(False)\n",
    "\n",
    "    trainable_modules = (\"attn1\")\n",
    "    for name, module in unet.named_modules():\n",
    "        if name.endswith(trainable_modules):\n",
    "            for params in module.parameters():\n",
    "                params.requires_grad = True\n",
    "        \n",
    "    if scale_lr:\n",
    "        learning_rate = (\n",
    "            learning_rate * gradient_accumulation_steps * train_batch_size * accelerator.num_processes\n",
    "        )\n",
    "\n",
    "    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\n",
    "    if use_8bit_adam:\n",
    "        try:\n",
    "            import bitsandbytes as bnb\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\"\n",
    "            )\n",
    "        optimizer_class = bnb.optim.AdamW8bit\n",
    "    else:\n",
    "        optimizer_class = torch.optim.AdamW\n",
    "\n",
    "    params_to_optimize = unet.parameters()\n",
    "    optimizer = optimizer_class(\n",
    "        params_to_optimize,\n",
    "        lr=learning_rate,\n",
    "        betas=(adam_beta1, adam_beta2),\n",
    "        weight_decay=adam_weight_decay,\n",
    "        eps=adam_epsilon,\n",
    "    )\n",
    "\n",
    "    train_dataset = StorySalonDataset(root=\"./StorySalon/\", dataset_name='train')\n",
    "    val_dataset = StorySalonDataset(root=\"./StorySalon/\", dataset_name='test')\n",
    "    \n",
    "    print(train_dataset.__len__())\n",
    "    print(val_dataset.__len__())\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=8)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        lr_scheduler,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=lr_warmup_steps * gradient_accumulation_steps,\n",
    "        num_training_steps=train_steps * gradient_accumulation_steps,\n",
    "    )\n",
    "\n",
    "    unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        unet, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    weight_dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        weight_dtype = torch.bfloat16\n",
    "\n",
    "    # For mixed precision training we cast the text_encoder and vae weights to half-precision\n",
    "    # as these models are only used for inference, keeping weights in full precision is not required.\n",
    "    vae.to(accelerator.device, dtype=weight_dtype)\n",
    "    text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "    \n",
    "    # We need to initialize the trackers we use, and also store our configuration.\n",
    "    # The trackers initializes automatically on the main process.\n",
    "    if accelerator.is_main_process:\n",
    "        accelerator.init_trackers(\"StoryGen-Single\")\n",
    "    step = 0\n",
    "\n",
    "    if validation_sample_logger is not None and accelerator.is_main_process:\n",
    "        validation_sample_logger = SampleLogger(**validation_sample_logger, logdir=logdir)\n",
    "\n",
    "    progress_bar = tqdm(range(step, train_steps), disable=not accelerator.is_local_main_process)\n",
    "    progress_bar.set_description(\"Steps\")\n",
    "\n",
    "    def make_data_yielder(dataloader):\n",
    "        while True:\n",
    "            for batch in dataloader:\n",
    "                yield batch\n",
    "            accelerator.wait_for_everyone()\n",
    "\n",
    "    train_data_yielder = make_data_yielder(train_dataloader)\n",
    "    val_data_yielder = make_data_yielder(val_dataloader)\n",
    "\n",
    "    while step < train_steps:\n",
    "        batch = next(train_data_yielder)\n",
    "        \n",
    "        vae.eval()\n",
    "        text_encoder.eval()\n",
    "        unet.train()\n",
    "        \n",
    "        image = batch[\"image\"].to(dtype=weight_dtype)\n",
    "        prompt = batch[\"prompt\"]\n",
    "        prompt_ids = tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=tokenizer.model_max_length, return_tensors=\"pt\").input_ids\n",
    "        mask = batch[\"mask\"].to(dtype=weight_dtype)        \n",
    "        mask = mask[:, [0], :, :].repeat(1, 4, 1, 1) # 3 channels to 4 channels\n",
    "        mask = F.interpolate(mask, scale_factor = 1 / 8., mode=\"bilinear\", align_corners=False)\n",
    "        b, c, h, w = image.shape\n",
    "        \n",
    "        latents = vae.encode(image).latent_dist.sample()\n",
    "        latents = latents * 0.18215\n",
    "        \n",
    "        # Sample noise that we'll add\n",
    "        noise = torch.randn_like(latents) # [-1, 1]\n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (b,), device=latents.device)\n",
    "        timesteps = timesteps.long()\n",
    "        \n",
    "        # Add noise according to the noise magnitude at each timestep (this is the forward diffusion process)\n",
    "        noisy_latent = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "        # Get the text embedding for conditioning\n",
    "        encoder_hidden_states = text_encoder(prompt_ids.to(accelerator.device))[0] # B * 77 * 768\n",
    "        \n",
    "        # Predict the noise residual\n",
    "        model_pred = unet(noisy_latent, timesteps, encoder_hidden_states=encoder_hidden_states, image_hidden_states=None, return_dict=False)[0]\n",
    "        \n",
    "        # loss = F.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
    "        loss = F.mse_loss(model_pred.float() * (1. - mask), noise.float() * (1 - mask), reduction=\"mean\")\n",
    "\n",
    "        accelerator.backward(loss)\n",
    "        if accelerator.sync_gradients:\n",
    "            accelerator.clip_grad_norm_(unet.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if accelerator.sync_gradients:\n",
    "            progress_bar.update(1)\n",
    "            step += 1\n",
    "            if accelerator.is_main_process:\n",
    "                if validation_sample_logger is not None and step % validation_steps == 0:\n",
    "                    unet.eval()\n",
    "                    val_batch = next(val_data_yielder)\n",
    "                    with autocast():\n",
    "                        validation_sample_logger.log_sample_images(\n",
    "                            batch = val_batch,\n",
    "                            pipeline=pipeline,\n",
    "                            device=accelerator.device,\n",
    "                            step=step,\n",
    "                        )\n",
    "                if step % checkpointing_steps == 0:\n",
    "                    pipeline_save = StableDiffusionPipeline(\n",
    "                        vae=vae,\n",
    "                        text_encoder=text_encoder,\n",
    "                        tokenizer=tokenizer,\n",
    "                        unet=accelerator.unwrap_model(unet),\n",
    "                        scheduler=scheduler,\n",
    "                    )\n",
    "                    checkpoint_save_path = os.path.join(logdir, f\"checkpoint_{step}\")\n",
    "                    pipeline_save.save_pretrained(checkpoint_save_path)\n",
    "\n",
    "        logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        accelerator.log(logs, step=step)\n",
    "    accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = \"/workspace/StoryGen/config/stage1_config_server.yml\"\n",
    "    train(**OmegaConf.load(config))\n",
    "\n",
    "# CUDA_VISIBLE_DEVICES=0,1 accelerate launch train_StorySalon.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "storygen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
